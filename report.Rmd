Classifying Proper Exercise From Sensory Data
========================================================

## Synopsis
Correct form is critical for reducing the risk of injury from weight-lifting exercises. An injury can negatively impact further training and led to less physical fitness and well-being. The ability to provide the user feedback on the correctness of their form during an exercise would provide a major benefit to a user. Using sensory data provided from a Microsoft Xbox connect we can classify whether a subject is correctly performing an exercise and classify the specific mistake if the repetition is not correct.[1] After data munging and  exploratory analysis I was able to build a model that provides very good prediction accuracy.

```{r echo=TRUE}
library(ggplot2)
suppressMessages(library(caret))
suppressMessages(library(randomForest))
# set R so that scientific notation less likely
options(scipen = 5)
```
```{r echo=FALSE}
### set work directory based on the machine I am using
if (file.exists("F:/machine_learning/assignments/ml_project"))  {
  setwd("f:/machine_learning/assignments/ml_project")
} else if (file.exists("E:/machine_learning/assignments/ml_project")){
  setwd("e:/machine_learning/assignments/ml_project")
} else {
  stop("no directory")
}
```


## Data Processing
Read in the data from the original csv file
```{r echo=TRUE,cache=TRUE}
# read the data
pml <- read.csv("./raw_data/pml-training.csv",stringsAsFactors=FALSE)
```
We use the nearZeroVar function from the caret package to eliminate variables that have low variability. These low variability predictors can cause some models to fail so we eliminate them before we start our model building. [2] We also eliminate some variables that are predominately NA and missing values. Finally we drop some variables related to time and user name. We are left with 52 predictors and the dependent variable of classe.
```{r echo=TRUE}
# eliminate variables with low variability 
# using the nearZeroVar function of the caret package
nz <- nearZeroVar(pml)
pml <- pml[,-nz]

# get rid of columns that are mostly NA
pml <- pml[,colSums(is.na(pml))/nrow(pml)<.9]

# get rid of user name and time related variables
pml <- pml[,-c(1:6)]

# make classe a factor variable
pml$classe <- factor(pml$classe)
```

Even though developers of the Random Forest algorithm say cross-validation is not necessary, we split the data into a training and testing (validation) sets. [3]
```{r echo=TRUE}
# split data into training and test sets
set.seed(77)
inTrain <- createDataPartition(y=pml$classe,p=0.6, list=FALSE)
training <- pml[inTrain,]
testing <- pml[-inTrain,]
```

## Model Selection and Results
The Random Forest classification method was chosen because it provided the best accuracy of any method I tried. I also tried K Nearest Neighbor and a tree model using the rpart package. The accuracy of the Random Foreest was very good with an accuracy rate of over 99% correct predictions on the testing data that was held out of the original dataset. As the contingency table/ confusion matrix show the model performs as well on the testing data as the training data. The plot for variable important is shown in figure 1 of the appendix.
```{r }
rf.fit <- randomForest(classe ~., data=training,type="class")
rfpred <- predict(rf.fit,newdata=testing)
rf.fit
rf.table <- table(rfpred,testing$classe)
```

Contingency Table for Training
```{r}
confusionMatrix(table(rf.fit$predicted,training$classe))
```

Contingency Table for Testing
```{r}
confusionMatrix(rf.table)
```

## Summary
Using the Random Forest method we were able to accurately classify the bicep curl exercise providing valuable insight for someone performing this exercise. As mentioned previously accuracy was over 99% for both training and testing data.

## Appendix
```{r Important Variables,fig.width=8, fig.height=6}
varImpPlot(rf.fit)
```

__Figure 1__ plots the variable importance for the Random Forest model.


# References
1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
2. Kuhn, Max. Building Predictive Models in R Using the caret Package. Journal of Statistical Software. Vol. 28. November 2008.
3. Breiman, L; Cutler, A; Random Forests. http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm